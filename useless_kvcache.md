Layer-Condensed KV Cache for Efficient Inference of Large Language Models
https://arxiv.org/pdf/2405.10637
You Only Cache Once: Decoder-Decoder Architectures for Language Model
https://arxiv.org/pdf/2405.05254
Reducing Transformer Key-Value Cache Size with Cross-Layer Attention
https://arxiv.org/pdf/2405.12981
